[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "assignment2",
    "section": "",
    "text": "Building a Qualtrics Panel\n\n(1) Use Google Trends website\n\n# Load required libraries\nlibrary(readr)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n# Read the CSV file without parsing dates\ntrends_data &lt;- read_csv(\"multiTimeline.csv\", skip = 2, col_names = FALSE, \n                        col_types = cols(.default = \"c\"))\n\n# Rename columns\ncolnames(trends_data) &lt;- c(\"Time\", \"Trump\", \"Kamala_Harris\", \"Election\")\n\n# Print first few rows to check the data\nprint(head(trends_data))\n\n# A tibble: 6 × 4\n  Time                      Trump                  Kamala_Harris        Election\n  &lt;chr&gt;                     &lt;chr&gt;                  &lt;chr&gt;                &lt;chr&gt;   \n1 Time                      Trump: (United States) Kamala Harris: (Uni… Electio…\n2 2024-09-28T12:04:00-05:00 45                     11                   10      \n3 2024-09-28T12:12:00-05:00 41                     11                   10      \n4 2024-09-28T12:20:00-05:00 40                     10                   10      \n5 2024-09-28T12:28:00-05:00 50                     11                   11      \n6 2024-09-28T12:36:00-05:00 43                     11                   9       \n\n# Convert Time column to proper datetime format\n# Adjust the format based on your actual data\ntrends_data$Time &lt;- parse_date_time(trends_data$Time, orders = c(\"mdy HM\", \"ymd HM\"))\n\nWarning: All formats failed to parse. No formats found.\n\n# Date analysis\nstart_time &lt;- min(trends_data$Time, na.rm = TRUE)\n\nWarning in min.default(structure(c(NA_real_, NA_real_, NA_real_, NA_real_, : no\nnon-missing arguments to min; returning Inf\n\nend_time &lt;- max(trends_data$Time, na.rm = TRUE)\n\nWarning in max.default(structure(c(NA_real_, NA_real_, NA_real_, NA_real_, : no\nnon-missing arguments to max; returning -Inf\n\ntime_range &lt;- as.numeric(difftime(end_time, start_time, units = \"hours\"))\n\n# Interval analysis\nnumber_of_datapoints &lt;- nrow(trends_data)\ninterval &lt;- as.numeric(difftime(trends_data$Time[2], trends_data$Time[1], units = \"hours\"))\n\n# Print results\ncat(\"Date Analysis:\\n\")\n\nDate Analysis:\n\ncat(\"Start Time:\", format(start_time, \"%Y-%m-%d %H:%M:%S\"), \"\\n\")\n\nStart Time: Inf \n\ncat(\"End Time:\", format(end_time, \"%Y-%m-%d %H:%M:%S\"), \"\\n\")\n\nEnd Time: -Inf \n\ncat(\"Time Range:\", time_range, \"hours\\n\\n\")\n\nTime Range: -Inf hours\n\ncat(\"Interval Analysis:\\n\")\n\nInterval Analysis:\n\ncat(\"Number of Data Points:\", number_of_datapoints, \"\\n\")\n\nNumber of Data Points: 182 \n\ncat(\"Interval between data points:\", interval, \"hours\\n\")\n\nInterval between data points: NA hours\n\n\n\n\n\n(2) Use gtrendsR package to collect data\n\n# Load library\nlibrary(gtrendsR)\n\n# Fetch data\nHarrisTrumpElection &lt;- gtrends(keyword = c(\"Trump\", \"Harris\", \"election\"), time = \"all\")\n\n# Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\nHarrisTrumpElection_interest &lt;- na.omit(HarrisTrumpElection_interest)\n\n# Plot data\npar(family=\"Georgia\")\nplot(HarrisTrumpElection_interest$date, HarrisTrumpElection_interest$hits, type=\"l\",\n     main=\"Google Trends: Trump, Harris, Election\", xlab=\"Date\", ylab=\"Search Interest\")\n\nWarning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion\n\n\n\n\n# Try another dataset\nborderimg &lt;- gtrends(keyword = c(\"border\", \"immigrant\"), time = \"all\")\n\n# Collect data by timeframe\nlast_hour &lt;- gtrends(keyword = c(\"Harris\", \"Trump\"), time = \"now 1-H\")\nlast_30_days &lt;- gtrends(keyword = c(\"Harris\", \"Trump\"), time = \"today 1-m\")\n\n# Collect data by country\ntg_gb &lt;- gtrends(keyword = c(\"immigrants\"), geo = c(\"GB\", \"US\"), time = \"all\")\n\n# Check country codes\ndata(\"countries\")\nhead(countries)\n\n  country_code sub_code        name\n1           AF     &lt;NA&gt; AFGHANISTAN\n2           AF   AF-BDS  BADAKHSHAN\n3           AF   AF-BDG     BADGHIS\n4           AF   AF-BGL     BAGHLAN\n5           AF   AF-BAL       BALKH\n6           AF   AF-BAM      BAMIAN\n\n\n\n\n\n(3). What are the differences between the two methods?\nThe Google Trends website offers a user-friendly interface for manual data collection via CSV downloads, making it suitable for casual use, but it has limitations in flexibility, automation, and precision. In contrast, the gtrendsR package in R provides programmatic API access, allowing automated, reproducible data collection with greater flexibility in parameter adjustments and time range specifications. While the website requires additional data cleaning and is less suited for systematic research, gtrendsR delivers data directly as R objects, making it more efficient for thorough data analysis in research projects."
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "assignment1",
    "section": "",
    "text": "Survey Link : https://utdallas.qualtrics.com/jfe/form/SV_5swnclalwY0ZIrk"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "My name is Sunmin Hong, and I’m a third year doctoral candidate in criminology and criminal justice program at UT Dallas.\n\nMore Information\nResearch Interests:\n\nCybercrimes\nCriminal Justice Policies\nEnvironmental Criminology\n\n\n\nContact me\n[sunmin.hong@utdallas.edu]"
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "assignment3",
    "section": "",
    "text": "****** Biden-Xi summit data **********\nHashtag Network Analysis: Central hashtags: #china, #xijinping, #biden Key issue clusters:\nHuman rights: #uyghurgenocide, #humanrights, #uyghurs Meeting topics: #biden, #xijinping, #china Regional issues: #taiwan Other concerns: #coronavirus, #fentanyl\nNotable connections: #china is centrally connected to #taiwan and #usa\nUser Mention Network Analysis:\nSeveral distinct user groups:\nNews media accounts: @politico, @foxbusiness NBA-related accounts: @nba, @pelicansnba, @capitalonearena Various individual user clusters\n******* US presidential inaugural speeches **********\nCommon Words Across Presidents:\nCore terms: “american”, “freedom”, “country”, “nation” Action words: “must”, “can” Collective terms: “people”, “us”\nPresidential Distinctions:\nBush: emphasis on “freedom” and “liberty” Obama: focus on “time”, “new”, and “change” Trump: strong emphasis on “america” and “american” Biden: emphasis on “democracy” and “nation”\n\nSimilarities and differences:\n\nSimilarities: All presidents consistently use patriotic language and emphasize unity Differences: Each president shows unique word preferences reflecting their era’s challenges and their political priorities Time Changes: Evolution from traditional values to modern global concerns visible in word choices\n***** Wordfish *******\nIt is a statistical model used in text analysis. Its key features and uses are as follows:\nBasic Concept:\nAn unsupervised scaling algorithm that estimates ideological positions from political texts Automatically estimates document positions based on word frequency patterns\nOperating Mechanism:\nAnalyzes relative differences in word usage patterns Models word frequency using Poisson distribution Places documents on a single dimension (typically ideological spectrum)\nMain Applications:\nAnalysis of political speeches Comparison of party manifestos Identifying ideological positions in policy documents Tracking changes in political discourse over time\nAdvantages:\nCan determine relative document positions without prior information Provides objective text analysis method Suitable for large-scale document analysis\nAnalysis Examples:\nAnalyzing ideological changes in presidential inaugural speeches Identifying differences in party policy documents Tracking changes in political discourse over time"
  },
  {
    "objectID": "assignment4.html",
    "href": "assignment4.html",
    "title": "assignment4",
    "section": "",
    "text": "Wikipedia Data Scraping Used rvest_wiki01.R to scrape foreign reserve data from Wikipedia. The process used R’s rvest package to extract table data from the webpage, then cleaned and processed the data.\nGovernment Documents Download Used govtdata01.R to download documents from govinfo.gov. Results: Successfully downloaded documents:\n\nCPRT-109JPRT25514 CPRT-115SPRT28545 CPRT-115SPRT23704\nFailed downloads:\n2 documents with valid URLs failed 5 documents had NA values in URL fields\n\nScraping Process Report\n\n\n\nData Usability\n\nWikipedia Data:\nSuccessfully extracted foreign reserves table Data properly structured with country names and values Dates successfully converted to proper format\nGovernment Documents:\n30% success rate in document retrieval (3 out of 10) Downloaded documents are usable PDF files Some URLs were inaccessible or invalid\n\nImprovement Suggestions\n\nFor Wikipedia Scraping:\nAdd error handling for table structure changes Implement automatic table detection Add data validation steps\nFor Government Documents:\nImplement URL validation before download Add retry mechanism for failed downloads Better error logging and reporting Alternative download methods for failed cases"
  },
  {
    "objectID": "assignment4.html#wikipedia-data-scraping",
    "href": "assignment4.html#wikipedia-data-scraping",
    "title": "assignment4",
    "section": "",
    "text": "Used rvest_wiki01.R to scrape foreign reserve data from Wikipedia. The process used R’s rvest package to extract table data from the webpage, then cleaned and processed the data."
  },
  {
    "objectID": "assignment4.html#government-documents-download",
    "href": "assignment4.html#government-documents-download",
    "title": "assignment4",
    "section": "2. Government Documents Download",
    "text": "2. Government Documents Download\nUsed govtdata01.R to download documents from govinfo.gov. Results:\nSuccessfully downloaded documents: 1. CPRT-109JPRT25514 2. CPRT-115SPRT28545 3. CPRT-115SPRT23704\nFailed downloads: - 2 documents with valid URLs failed - 5 documents had NA values in URL fields"
  },
  {
    "objectID": "assignment4.html#scraping-process-report",
    "href": "assignment4.html#scraping-process-report",
    "title": "assignment4",
    "section": "3. Scraping Process Report",
    "text": "3. Scraping Process Report\n\na. Data Usability\n\nWikipedia Data:\n\nSuccessfully extracted foreign reserves table\nData properly structured with country names and values\nDates successfully converted to proper format\n\nGovernment Documents:\n\n30% success rate in document retrieval (3 out of 10)\nDownloaded documents are usable PDF files\nSome URLs were inaccessible or invalid\n\n\n\n\nb. Improvement Suggestions\n\nFor Wikipedia Scraping:\n\nAdd error handling for table structure changes\nImplement automatic table detection\nAdd data validation steps\n\nFor Government Documents:\n\nImplement URL validation before download\nAdd retry mechanism for failed downloads\nBetter error logging and reporting\nAlternative download methods for failed cases"
  }
]